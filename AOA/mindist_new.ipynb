{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53f83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neighbors import KDTree\n",
    "import math\n",
    "import faiss\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f15add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns to use\n",
    "columns_order = [\n",
    "    'IMPERV', 'HEIGHT', 'COAST', 'ELEV', 'POP',  \n",
    "    'RH', 'SP', 'PRECIP', 'T_2M_COR', 'WS', 'TCC',  \n",
    "    'CAPE', 'BLH', 'SSR', 'SOLAR_ELEV', 'DECL'\n",
    "]\n",
    "\n",
    "# Load min-max scaling info and ensure correct matching\n",
    "min_max_df = pd.read_csv(\"data/CLUSTER2_min_max.csv\", sep=';')\n",
    "min_max_df.set_index(min_max_df.columns[0], inplace=True)\n",
    "\n",
    "# Ensure all required columns exist in the min-max file\n",
    "missing_cols = [col for col in columns_order if col not in min_max_df.index]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"The following required columns are missing in CLUSTER2_min_max.csv: {missing_cols}\")\n",
    "\n",
    "# Get min/max values in the same order as columns_order\n",
    "min_vals = min_max_df.loc[columns_order, 'min'].astype(float).to_numpy()\n",
    "max_vals = min_max_df.loc[columns_order, 'Max'].astype(float).to_numpy()\n",
    "\n",
    "# Avoid division by zero\n",
    "scale = max_vals - min_vals\n",
    "scale[scale == 0] = 1.0\n",
    "\n",
    "def min_max_scale(data, min_vals, scale):\n",
    "    return (data - min_vals) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f781123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load importances\n",
    "importances_df = pd.read_csv('AOA_data/importances_CL2.csv')\n",
    "importances_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Map feature names to column names in test_scaled\n",
    "importance_to_column_map = {\n",
    "    'IMPERV': 'IMPERV',\n",
    "    'HEIGHT': 'HEIGHT',\n",
    "    'COAST': 'COAST',\n",
    "    'ELEV': 'ELEV',\n",
    "    'POP': 'POP',\n",
    "    'RH': 'RH',\n",
    "    'SP': 'SP',\n",
    "    'PRECIP': 'PRECIP',\n",
    "    'T_2M': 'T_2M_COR',  # Important mapping\n",
    "    'wind_speed': 'WS',\n",
    "    'TCC': 'TCC',\n",
    "    'CAPE': 'CAPE',\n",
    "    'BLH': 'BLH',\n",
    "    'SSR': 'SSR',\n",
    "    'SOLAR_ELEV': 'SOLAR_ELEV',\n",
    "    'DECL': 'DECL'\n",
    "}\n",
    "\n",
    "# Build weights array in the same order as test_scaled columns\n",
    "weights = np.array([\n",
    "    importances_df.loc[feature, 'Importance'] \n",
    "    for feature in importance_to_column_map \n",
    "    if importance_to_column_map[feature] in columns_order\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebd1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data ready, working on trees now\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "test_file = 'data/CLUSTER2_VALIDATION_cleaned_cities.csv'\n",
    "\n",
    "# Define numerical and categorical structure\n",
    "columns_order = [\n",
    "    'IMPERV', 'HEIGHT', 'COAST', 'ELEV', 'POP',  \n",
    "    'RH', 'SP', 'PRECIP', 'T_2M_COR', 'WS', 'TCC',  \n",
    "    'CAPE', 'BLH', 'SSR', 'SOLAR_ELEV', 'DECL'\n",
    "]\n",
    "\n",
    "# Load test data with LC_CORINE\n",
    "test_df = pd.read_csv(test_file, usecols=columns_order + ['LC_CORINE'])\n",
    "test_scaled = min_max_scale(test_df[columns_order].to_numpy(), min_vals, scale)\n",
    "test_df_scaled = pd.DataFrame(test_scaled, columns=columns_order)\n",
    "test_df_scaled['LC_CORINE'] = test_df['LC_CORINE'].values\n",
    "print('test data ready, working on trees now')\n",
    "\n",
    "# Build KDTree for each LC_CORINE class\n",
    "trees_by_class = {}\n",
    "for lc_class in range(1, 16):\n",
    "    class_subset = test_df_scaled[test_df_scaled['LC_CORINE'] == lc_class]\n",
    "    if len(class_subset) == 0:\n",
    "        print(lc_class)\n",
    "        continue  # No test points for this class\n",
    "    features = class_subset[columns_order].to_numpy() * weights  # Apply weights\n",
    "    trees_by_class[lc_class] = KDTree(features, leaf_size=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf1cbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Build KDTree for each LC_CORINE class\n",
    "trees_by_class = {}\n",
    "for lc_class in range(1, 16):\n",
    "    class_subset = test_df_scaled[test_df_scaled['LC_CORINE'] == lc_class]\n",
    "    if len(class_subset) == 0:\n",
    "        print(lc_class)\n",
    "        continue  # No test points for this class\n",
    "    features = class_subset[columns_order].to_numpy() * weights  # Apply weights\n",
    "    trees_by_class[lc_class] = KDTree(features, leaf_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff1261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TRAIN data in chunks...\n",
      "Chunk 1 processed in 172.85s\n",
      "Chunk 2 processed in 250.60s\n",
      "Chunk 3 processed in 291.53s\n",
      "Chunk 4 processed in 313.38s\n",
      "Chunk 5 processed in 287.45s\n",
      "Chunk 6 processed in 211.31s\n",
      "Chunk 7 processed in 186.65s\n",
      "Chunk 8 processed in 187.85s\n",
      "Chunk 9 processed in 238.76s\n",
      "Chunk 10 processed in 299.55s\n",
      "Chunk 11 processed in 317.57s\n",
      "Chunk 12 processed in 281.06s\n",
      "Chunk 13 processed in 227.12s\n",
      "Chunk 14 processed in 193.85s\n",
      "Chunk 15 processed in 195.46s\n",
      "Chunk 16 processed in 249.46s\n",
      "Chunk 17 processed in 295.59s\n",
      "Chunk 18 processed in 325.51s\n",
      "Chunk 19 processed in 277.30s\n",
      "Chunk 20 processed in 208.01s\n",
      "Chunk 21 processed in 194.00s\n",
      "Chunk 22 processed in 575.34s\n",
      "Chunk 23 processed in 394.70s\n",
      "Chunk 24 processed in 278.74s\n",
      "Chunk 25 processed in 332.37s\n",
      "Chunk 26 processed in 298.28s\n",
      "Chunk 27 processed in 213.30s\n",
      "Chunk 28 processed in 203.83s\n",
      "Chunk 29 processed in 214.95s\n",
      "Chunk 30 processed in 260.81s\n",
      "Chunk 31 processed in 313.52s\n",
      "Chunk 32 processed in 336.42s\n",
      "Chunk 33 processed in 278.76s\n",
      "Chunk 34 processed in 222.46s\n",
      "Chunk 35 processed in 184.03s\n",
      "Chunk 36 processed in 195.74s\n",
      "Chunk 37 processed in 237.19s\n",
      "Chunk 38 processed in 288.42s\n",
      "Chunk 39 processed in 329.68s\n",
      "Chunk 40 processed in 300.84s\n",
      "Chunk 41 processed in 227.88s\n",
      "Chunk 42 processed in 200.20s\n",
      "Chunk 43 processed in 211.06s\n",
      "Chunk 44 processed in 222.73s\n",
      "Chunk 45 processed in 303.22s\n",
      "Chunk 46 processed in 313.29s\n",
      "Chunk 47 processed in 282.97s\n",
      "Chunk 48 processed in 211.38s\n",
      "Chunk 49 processed in 183.95s\n",
      "Chunk 50 processed in 199.69s\n",
      "Chunk 51 processed in 218.92s\n",
      "Chunk 52 processed in 272.40s\n",
      "Chunk 53 processed in 289.36s\n",
      "Chunk 54 processed in 282.88s\n",
      "Chunk 55 processed in 213.81s\n",
      "Chunk 56 processed in 173.98s\n",
      "Chunk 57 processed in 166.79s\n",
      "Chunk 58 processed in 227.21s\n",
      "Chunk 59 processed in 298.87s\n",
      "Chunk 60 processed in 275.99s\n",
      "Chunk 61 processed in 298.35s\n",
      "Chunk 62 processed in 217.31s\n",
      "Chunk 63 processed in 191.16s\n",
      "Chunk 64 processed in 177.55s\n",
      "Chunk 65 processed in 224.77s\n",
      "Chunk 66 processed in 278.42s\n",
      "Chunk 67 processed in 307.93s\n",
      "Chunk 68 processed in 311.41s\n",
      "Chunk 69 processed in 211.86s\n",
      "Chunk 70 processed in 194.35s\n",
      "Chunk 71 processed in 191.26s\n",
      "Chunk 72 processed in 233.02s\n",
      "Chunk 73 processed in 293.53s\n",
      "Chunk 74 processed in 330.04s\n",
      "Chunk 75 processed in 287.25s\n",
      "Chunk 76 processed in 233.16s\n",
      "Chunk 77 processed in 199.64s\n",
      "Chunk 78 processed in 188.72s\n",
      "Chunk 79 processed in 221.23s\n",
      "Chunk 80 processed in 255.13s\n",
      "Chunk 81 processed in 308.21s\n",
      "Chunk 82 processed in 312.92s\n",
      "Chunk 83 processed in 235.82s\n",
      "Chunk 84 processed in 193.36s\n",
      "Chunk 85 processed in 104.07s\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/CLUSTER2_TRAIN_cleaned_cities.csv'\n",
    "\n",
    "distances_list = []\n",
    "chunk_size = 1_000_000\n",
    "print(\"Processing TRAIN data in chunks...\")\n",
    "chunk_idx = 0\n",
    "chunk_start_time = time.time()\n",
    "\n",
    "for chunk in pd.read_csv(train_file, usecols=columns_order + ['LC_CORINE'], chunksize=chunk_size):\n",
    "    chunk_idx += 1\n",
    "\n",
    "    # Scale numerical features\n",
    "    chunk_scaled = min_max_scale(chunk[columns_order].to_numpy(), min_vals, scale)\n",
    "    weighted_features = chunk_scaled * weights\n",
    "    lc_values = chunk['LC_CORINE'].values\n",
    "\n",
    "    # Initialize distances for this chunk\n",
    "    chunk_dists = np.full(len(chunk), np.inf)\n",
    "\n",
    "    # For each class in this chunk, query the corresponding KDTree\n",
    "    for lc_class in np.unique(lc_values):\n",
    "        if lc_class not in trees_by_class:\n",
    "            print('oops')\n",
    "            continue  # No matching test points\n",
    "\n",
    "        mask = lc_values == lc_class\n",
    "        query_points = weighted_features[mask]\n",
    "\n",
    "        # Query KDTree\n",
    "        dists, _ = trees_by_class[lc_class].query(query_points, k=1)\n",
    "        chunk_dists[mask] = dists.flatten()\n",
    "\n",
    "    # Save distances\n",
    "    distances_list.append(pd.DataFrame(chunk_dists, columns=['dist']))\n",
    "\n",
    "    duration = time.time() - chunk_start_time\n",
    "    print(f\"Chunk {chunk_idx} processed in {duration:.2f}s\")\n",
    "    chunk_start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc93bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All distances written to: results/CLUSTER2_TRAIN_min_dist.csv\n"
     ]
    }
   ],
   "source": [
    "# Write distances to CSV\n",
    "all_distances = pd.concat(distances_list, ignore_index=True)\n",
    "output_file = 'results/CLUSTER2_TRAIN_min_dist.csv'\n",
    "all_distances.to_csv(output_file, index=False)\n",
    "print(f\"\\nAll distances written to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
