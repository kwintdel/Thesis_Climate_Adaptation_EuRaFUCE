{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53f83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neighbors import KDTree\n",
    "import math\n",
    "import faiss\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f15add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns to use\n",
    "columns_order = [\n",
    "    'IMPERV', 'HEIGHT', 'COAST', 'ELEV', 'POP',  \n",
    "    'RH', 'SP', 'PRECIP', 'T_2M_COR', 'WS', 'TCC',  \n",
    "    'CAPE', 'BLH', 'SSR', 'SOLAR_ELEV', 'DECL'\n",
    "]\n",
    "\n",
    "# Load min-max scaling info and ensure correct matching\n",
    "min_max_df = pd.read_csv(\"data/CLUSTER1_min_max.csv\", sep=';')\n",
    "min_max_df.set_index(min_max_df.columns[0], inplace=True)\n",
    "\n",
    "# Ensure all required columns exist in the min-max file\n",
    "missing_cols = [col for col in columns_order if col not in min_max_df.index]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"The following required columns are missing in CLUSTER1_min_max.csv: {missing_cols}\")\n",
    "\n",
    "# Get min/max values in the same order as columns_order\n",
    "min_vals = min_max_df.loc[columns_order, 'min'].astype(float).to_numpy()\n",
    "max_vals = min_max_df.loc[columns_order, 'Max'].astype(float).to_numpy()\n",
    "\n",
    "# Avoid division by zero\n",
    "scale = max_vals - min_vals\n",
    "scale[scale == 0] = 1.0\n",
    "\n",
    "def min_max_scale(data, min_vals, scale):\n",
    "    return (data - min_vals) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f781123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load importances\n",
    "importances_df = pd.read_csv('AOA_data/importances_CL1.csv')\n",
    "importances_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Map feature names to column names in test_scaled\n",
    "importance_to_column_map = {\n",
    "    'IMPERV': 'IMPERV',\n",
    "    'HEIGHT': 'HEIGHT',\n",
    "    'COAST': 'COAST',\n",
    "    'ELEV': 'ELEV',\n",
    "    'POP': 'POP',\n",
    "    'RH': 'RH',\n",
    "    'SP': 'SP',\n",
    "    'PRECIP': 'PRECIP',\n",
    "    'T_2M': 'T_2M_COR',  # Important mapping\n",
    "    'wind_speed': 'WS',\n",
    "    'TCC': 'TCC',\n",
    "    'CAPE': 'CAPE',\n",
    "    'BLH': 'BLH',\n",
    "    'SSR': 'SSR',\n",
    "    'SOLAR_ELEV': 'SOLAR_ELEV',\n",
    "    'DECL': 'DECL'\n",
    "}\n",
    "\n",
    "# Build weights array in the same order as test_scaled columns\n",
    "weights = np.array([\n",
    "    importances_df.loc[feature, 'Importance'] \n",
    "    for feature in importance_to_column_map \n",
    "    if importance_to_column_map[feature] in columns_order\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ebd1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data ready, working on trees now\n",
      "5\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "test_file = 'data/CLUSTER1_VALIDATION_cleaned_cities.csv'\n",
    "\n",
    "# Define numerical and categorical structure\n",
    "columns_order = [\n",
    "    'IMPERV', 'HEIGHT', 'COAST', 'ELEV', 'POP',  \n",
    "    'RH', 'SP', 'PRECIP', 'T_2M_COR', 'WS', 'TCC',  \n",
    "    'CAPE', 'BLH', 'SSR', 'SOLAR_ELEV', 'DECL'\n",
    "]\n",
    "\n",
    "# Load test data with LC_CORINE\n",
    "test_df = pd.read_csv(test_file, usecols=columns_order + ['LC_CORINE'])\n",
    "test_scaled = min_max_scale(test_df[columns_order].to_numpy(), min_vals, scale)\n",
    "test_df_scaled = pd.DataFrame(test_scaled, columns=columns_order)\n",
    "test_df_scaled['LC_CORINE'] = test_df['LC_CORINE'].values\n",
    "print('test data ready, working on trees now')\n",
    "\n",
    "# Build KDTree for each LC_CORINE class\n",
    "trees_by_class = {}\n",
    "for lc_class in range(1, 16):\n",
    "    class_subset = test_df_scaled[test_df_scaled['LC_CORINE'] == lc_class]\n",
    "    if len(class_subset) == 0:\n",
    "        print(lc_class)\n",
    "        continue  # No test points for this class\n",
    "    features = class_subset[columns_order].to_numpy() * weights  # Apply weights\n",
    "    trees_by_class[lc_class] = KDTree(features, leaf_size=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff1261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TRAIN data in chunks...\n",
      "Chunk 1 processed in 500.79s\n",
      "Chunk 2 processed in 429.11s\n",
      "Chunk 3 processed in 450.97s\n",
      "Chunk 4 processed in 468.82s\n",
      "Chunk 5 processed in 489.50s\n",
      "Chunk 6 processed in 531.85s\n",
      "Chunk 7 processed in 443.79s\n",
      "Chunk 8 processed in 397.25s\n",
      "Chunk 9 processed in 408.10s\n",
      "Chunk 10 processed in 491.79s\n",
      "Chunk 11 processed in 417.80s\n",
      "Chunk 12 processed in 486.62s\n",
      "Chunk 13 processed in 528.62s\n",
      "Chunk 14 processed in 484.80s\n",
      "Chunk 15 processed in 457.52s\n",
      "Chunk 16 processed in 466.49s\n",
      "Chunk 17 processed in 505.15s\n",
      "Chunk 18 processed in 477.55s\n",
      "Chunk 19 processed in 457.33s\n",
      "Chunk 20 processed in 484.62s\n",
      "Chunk 21 processed in 463.89s\n",
      "Chunk 22 processed in 448.71s\n",
      "Chunk 23 processed in 442.47s\n",
      "Chunk 24 processed in 404.98s\n",
      "Chunk 25 processed in 401.69s\n",
      "Chunk 26 processed in 437.74s\n",
      "Chunk 27 processed in 379.44s\n",
      "Chunk 28 processed in 366.33s\n",
      "Chunk 29 processed in 383.80s\n",
      "Chunk 30 processed in 535.46s\n",
      "Chunk 31 processed in 441.09s\n",
      "Chunk 32 processed in 381.19s\n",
      "Chunk 33 processed in 450.52s\n",
      "Chunk 34 processed in 433.62s\n",
      "Chunk 35 processed in 472.83s\n",
      "Chunk 36 processed in 566.52s\n",
      "Chunk 37 processed in 574.45s\n",
      "Chunk 38 processed in 504.61s\n",
      "Chunk 39 processed in 527.53s\n",
      "Chunk 40 processed in 469.42s\n",
      "Chunk 41 processed in 421.95s\n",
      "Chunk 42 processed in 450.89s\n",
      "Chunk 43 processed in 462.06s\n",
      "Chunk 44 processed in 440.60s\n",
      "Chunk 45 processed in 432.54s\n",
      "Chunk 46 processed in 429.05s\n",
      "Chunk 47 processed in 443.37s\n",
      "Chunk 48 processed in 426.05s\n",
      "Chunk 49 processed in 473.56s\n",
      "Chunk 50 processed in 508.55s\n",
      "Chunk 51 processed in 480.51s\n",
      "Chunk 52 processed in 427.88s\n",
      "Chunk 53 processed in 480.96s\n",
      "Chunk 54 processed in 478.99s\n",
      "Chunk 55 processed in 403.70s\n",
      "Chunk 56 processed in 388.37s\n",
      "Chunk 57 processed in 427.02s\n",
      "Chunk 58 processed in 354.85s\n",
      "Chunk 59 processed in 334.60s\n",
      "Chunk 60 processed in 399.56s\n",
      "Chunk 61 processed in 361.66s\n",
      "Chunk 62 processed in 367.63s\n",
      "Chunk 63 processed in 369.97s\n",
      "Chunk 64 processed in 345.03s\n",
      "Chunk 65 processed in 354.41s\n",
      "Chunk 66 processed in 377.79s\n",
      "Chunk 67 processed in 375.35s\n",
      "Chunk 68 processed in 396.84s\n",
      "Chunk 69 processed in 381.73s\n",
      "Chunk 70 processed in 438.21s\n",
      "Chunk 71 processed in 425.35s\n",
      "Chunk 72 processed in 402.42s\n",
      "Chunk 73 processed in 433.07s\n",
      "Chunk 74 processed in 423.37s\n",
      "Chunk 75 processed in 372.69s\n",
      "Chunk 76 processed in 409.92s\n",
      "Chunk 77 processed in 449.94s\n",
      "Chunk 78 processed in 414.41s\n",
      "Chunk 79 processed in 337.68s\n",
      "Chunk 80 processed in 367.17s\n",
      "Chunk 81 processed in 339.91s\n",
      "Chunk 82 processed in 358.42s\n",
      "Chunk 83 processed in 344.82s\n",
      "Chunk 84 processed in 455.01s\n",
      "Chunk 85 processed in 441.84s\n",
      "Chunk 86 processed in 440.89s\n",
      "Chunk 87 processed in 409.45s\n",
      "Chunk 88 processed in 415.57s\n",
      "Chunk 89 processed in 386.17s\n",
      "Chunk 90 processed in 446.86s\n",
      "Chunk 91 processed in 440.61s\n",
      "Chunk 92 processed in 405.44s\n",
      "Chunk 93 processed in 396.43s\n",
      "Chunk 94 processed in 388.10s\n",
      "Chunk 95 processed in 339.14s\n",
      "Chunk 96 processed in 396.74s\n",
      "Chunk 97 processed in 378.99s\n",
      "Chunk 98 processed in 315.57s\n",
      "Chunk 99 processed in 351.57s\n",
      "Chunk 100 processed in 357.43s\n",
      "Chunk 101 processed in 320.37s\n",
      "Chunk 102 processed in 383.33s\n",
      "Chunk 103 processed in 355.85s\n",
      "Chunk 104 processed in 414.80s\n",
      "Chunk 105 processed in 442.16s\n",
      "Chunk 106 processed in 470.51s\n",
      "Chunk 107 processed in 458.91s\n",
      "Chunk 108 processed in 461.13s\n",
      "Chunk 109 processed in 471.78s\n",
      "Chunk 110 processed in 423.88s\n",
      "Chunk 111 processed in 448.93s\n",
      "Chunk 112 processed in 416.14s\n",
      "Chunk 113 processed in 439.15s\n",
      "Chunk 114 processed in 382.67s\n",
      "Chunk 115 processed in 421.66s\n",
      "Chunk 116 processed in 448.82s\n",
      "Chunk 117 processed in 431.32s\n",
      "Chunk 118 processed in 421.56s\n",
      "Chunk 119 processed in 457.08s\n",
      "Chunk 120 processed in 491.25s\n",
      "Chunk 121 processed in 470.78s\n",
      "Chunk 122 processed in 555.23s\n",
      "Chunk 123 processed in 539.83s\n",
      "Chunk 124 processed in 419.83s\n",
      "Chunk 125 processed in 486.42s\n",
      "Chunk 126 processed in 483.27s\n",
      "Chunk 127 processed in 375.61s\n",
      "Chunk 128 processed in 389.84s\n",
      "Chunk 129 processed in 417.67s\n",
      "Chunk 130 processed in 378.53s\n",
      "Chunk 131 processed in 408.80s\n",
      "Chunk 132 processed in 359.83s\n",
      "Chunk 133 processed in 337.17s\n",
      "Chunk 134 processed in 338.51s\n",
      "Chunk 135 processed in 321.11s\n",
      "Chunk 136 processed in 401.19s\n",
      "Chunk 137 processed in 363.16s\n",
      "Chunk 138 processed in 369.06s\n",
      "Chunk 139 processed in 480.66s\n",
      "Chunk 140 processed in 473.48s\n",
      "Chunk 141 processed in 424.86s\n",
      "Chunk 142 processed in 459.28s\n",
      "Chunk 143 processed in 488.42s\n",
      "Chunk 144 processed in 406.39s\n",
      "Chunk 145 processed in 407.83s\n",
      "Chunk 146 processed in 413.39s\n",
      "Chunk 147 processed in 406.97s\n",
      "Chunk 148 processed in 417.58s\n",
      "Chunk 149 processed in 464.19s\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/CLUSTER1_TRAIN_cleaned_cities.csv'\n",
    "output_file = 'results/CLUSTER1_TRAIN_min_dist.csv' # Output file with distances\n",
    "\n",
    "chunk_size = 1_000_000\n",
    "print(\"Processing TRAIN data in chunks...\")\n",
    "chunk_idx = 0\n",
    "chunk_start_time = time.time()\n",
    "\n",
    "# Get number of already processed rows in output_file (excluding header)\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file) as f:\n",
    "        processed_rows = sum(1 for _ in f) - 1  # subtract header\n",
    "        del f\n",
    "else:\n",
    "    processed_rows = 0\n",
    "\n",
    "# Chunked reading of input file, skipping already processed rows\n",
    "for chunk in pd.read_csv(train_file, usecols=columns_order + ['LC_CORINE'],\n",
    "                         chunksize=chunk_size, skiprows=range(1, processed_rows + 1),\n",
    "                         header=0):\n",
    "    chunk_idx += 1\n",
    "\n",
    "    # Scale numerical features\n",
    "    chunk_scaled = min_max_scale(chunk[columns_order].to_numpy(), min_vals, scale)\n",
    "    weighted_features = chunk_scaled * weights\n",
    "    lc_values = chunk['LC_CORINE'].values\n",
    "\n",
    "    # Initialize distances for this chunk\n",
    "    chunk_dists = np.full(len(chunk), np.inf)\n",
    "\n",
    "    # For each class in this chunk, query the corresponding KDTree\n",
    "    for lc_class in np.unique(lc_values):\n",
    "        if lc_class not in trees_by_class:\n",
    "            print('oops')\n",
    "            continue\n",
    "\n",
    "        mask = lc_values == lc_class\n",
    "        query_points = weighted_features[mask]\n",
    "\n",
    "        # Query KDTree\n",
    "        dists, _ = trees_by_class[lc_class].query(query_points, k=1)\n",
    "        chunk_dists[mask] = dists.flatten()\n",
    "\n",
    "    # Add distances as a new column\n",
    "    chunk['dist'] = chunk_dists\n",
    "\n",
    "    # Append to the output CSV\n",
    "    chunk.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n",
    "\n",
    "    duration = time.time() - chunk_start_time\n",
    "    print(f\"Chunk {chunk_idx} processed in {duration:.2f}s\")\n",
    "    chunk_start_time = time.time()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
