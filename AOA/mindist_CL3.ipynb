{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53f83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neighbors import KDTree\n",
    "import math\n",
    "import faiss\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f15add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns to use\n",
    "columns_order = [\n",
    "    'IMPERV', 'HEIGHT', 'COAST', 'ELEV', 'POP',  \n",
    "    'RH', 'SP', 'PRECIP', 'T_2M_COR', 'WS', 'TCC',  \n",
    "    'CAPE', 'BLH', 'SSR', 'SOLAR_ELEV', 'DECL'\n",
    "]\n",
    "\n",
    "# Load min-max scaling info and ensure correct matching\n",
    "min_max_df = pd.read_csv(\"data/CLUSTER3_min_max.csv\", sep=';')\n",
    "min_max_df.set_index(min_max_df.columns[0], inplace=True)\n",
    "\n",
    "# Ensure all required columns exist in the min-max file\n",
    "missing_cols = [col for col in columns_order if col not in min_max_df.index]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"The following required columns are missing in CLUSTER3_min_max.csv: {missing_cols}\")\n",
    "\n",
    "# Get min/max values in the same order as columns_order\n",
    "min_vals = min_max_df.loc[columns_order, 'min'].astype(float).to_numpy()\n",
    "max_vals = min_max_df.loc[columns_order, 'Max'].astype(float).to_numpy()\n",
    "\n",
    "# Avoid division by zero\n",
    "scale = max_vals - min_vals\n",
    "scale[scale == 0] = 1.0\n",
    "\n",
    "def min_max_scale(data, min_vals, scale):\n",
    "    return (data - min_vals) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f781123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load importances\n",
    "importances_df = pd.read_csv('AOA_data/importances_CL3.csv')\n",
    "importances_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Map feature names to column names in test_scaled\n",
    "importance_to_column_map = {\n",
    "    'IMPERV': 'IMPERV',\n",
    "    'HEIGHT': 'HEIGHT',\n",
    "    'COAST': 'COAST',\n",
    "    'ELEV': 'ELEV',\n",
    "    'POP': 'POP',\n",
    "    'RH': 'RH',\n",
    "    'SP': 'SP',\n",
    "    'PRECIP': 'PRECIP',\n",
    "    'T_2M': 'T_2M_COR',  # Important mapping\n",
    "    'wind_speed': 'WS',\n",
    "    'TCC': 'TCC',\n",
    "    'CAPE': 'CAPE',\n",
    "    'BLH': 'BLH',\n",
    "    'SSR': 'SSR',\n",
    "    'SOLAR_ELEV': 'SOLAR_ELEV',\n",
    "    'DECL': 'DECL'\n",
    "}\n",
    "\n",
    "# Build weights array in the same order as test_scaled columns\n",
    "weights = np.array([\n",
    "    importances_df.loc[feature, 'Importance'] \n",
    "    for feature in importance_to_column_map \n",
    "    if importance_to_column_map[feature] in columns_order\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ebd1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data ready, working on trees now\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "test_file = 'data/CLUSTER3_VALIDATION_cleaned_cities.csv'\n",
    "\n",
    "# Define numerical and categorical structure\n",
    "columns_order = [\n",
    "    'IMPERV', 'HEIGHT', 'COAST', 'ELEV', 'POP',  \n",
    "    'RH', 'SP', 'PRECIP', 'T_2M_COR', 'WS', 'TCC',  \n",
    "    'CAPE', 'BLH', 'SSR', 'SOLAR_ELEV', 'DECL'\n",
    "]\n",
    "\n",
    "# Load test data with LC_CORINE\n",
    "test_df = pd.read_csv(test_file, usecols=columns_order + ['LC_CORINE'])\n",
    "test_scaled = min_max_scale(test_df[columns_order].to_numpy(), min_vals, scale)\n",
    "test_df_scaled = pd.DataFrame(test_scaled, columns=columns_order)\n",
    "test_df_scaled['LC_CORINE'] = test_df['LC_CORINE'].values\n",
    "print('test data ready, working on trees now')\n",
    "\n",
    "# Build KDTree for each LC_CORINE class\n",
    "trees_by_class = {}\n",
    "for lc_class in range(1, 16):\n",
    "    class_subset = test_df_scaled[test_df_scaled['LC_CORINE'] == lc_class]\n",
    "    if len(class_subset) == 0:\n",
    "        print(lc_class)\n",
    "        continue  # No test points for this class\n",
    "    features = class_subset[columns_order].to_numpy() * weights  # Apply weights\n",
    "    trees_by_class[lc_class] = KDTree(features, leaf_size=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff1261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TRAIN data in chunks...\n",
      "Chunk 1 processed in 319.42s\n",
      "Chunk 2 processed in 336.29s\n",
      "Chunk 3 processed in 374.01s\n",
      "Chunk 4 processed in 332.08s\n",
      "Chunk 5 processed in 388.23s\n",
      "Chunk 6 processed in 394.34s\n",
      "Chunk 7 processed in 367.44s\n",
      "Chunk 8 processed in 346.32s\n",
      "Chunk 9 processed in 289.43s\n",
      "Chunk 10 processed in 278.19s\n",
      "Chunk 11 processed in 281.28s\n",
      "Chunk 12 processed in 265.24s\n",
      "Chunk 13 processed in 358.34s\n",
      "Chunk 14 processed in 357.71s\n",
      "Chunk 15 processed in 431.86s\n",
      "Chunk 16 processed in 380.23s\n",
      "Chunk 17 processed in 394.37s\n",
      "Chunk 18 processed in 362.25s\n",
      "Chunk 19 processed in 353.45s\n",
      "Chunk 20 processed in 365.60s\n",
      "Chunk 21 processed in 324.94s\n",
      "Chunk 22 processed in 315.83s\n",
      "Chunk 23 processed in 416.01s\n",
      "Chunk 24 processed in 397.36s\n",
      "Chunk 25 processed in 407.20s\n",
      "Chunk 26 processed in 404.10s\n",
      "Chunk 27 processed in 395.61s\n",
      "Chunk 28 processed in 355.89s\n",
      "Chunk 29 processed in 290.30s\n",
      "Chunk 30 processed in 318.77s\n",
      "Chunk 31 processed in 333.41s\n",
      "Chunk 32 processed in 308.91s\n",
      "Chunk 33 processed in 312.69s\n",
      "Chunk 34 processed in 323.33s\n",
      "Chunk 35 processed in 391.90s\n",
      "Chunk 36 processed in 409.58s\n",
      "Chunk 37 processed in 391.72s\n",
      "Chunk 38 processed in 326.49s\n",
      "Chunk 39 processed in 315.02s\n",
      "Chunk 40 processed in 349.64s\n",
      "Chunk 41 processed in 336.65s\n",
      "Chunk 42 processed in 301.16s\n",
      "Chunk 43 processed in 364.44s\n",
      "Chunk 44 processed in 359.03s\n",
      "Chunk 45 processed in 487.59s\n",
      "Chunk 46 processed in 402.84s\n",
      "Chunk 47 processed in 397.22s\n",
      "Chunk 48 processed in 355.28s\n",
      "Chunk 49 processed in 311.28s\n",
      "Chunk 50 processed in 328.29s\n",
      "Chunk 51 processed in 264.76s\n",
      "Chunk 52 processed in 259.40s\n",
      "Chunk 53 processed in 316.18s\n",
      "Chunk 54 processed in 319.79s\n",
      "Chunk 55 processed in 440.35s\n",
      "Chunk 56 processed in 403.98s\n",
      "Chunk 57 processed in 405.74s\n",
      "Chunk 58 processed in 354.62s\n",
      "Chunk 59 processed in 323.30s\n",
      "Chunk 60 processed in 389.41s\n",
      "Chunk 61 processed in 318.35s\n",
      "Chunk 62 processed in 304.07s\n",
      "Chunk 63 processed in 301.19s\n",
      "Chunk 64 processed in 347.96s\n",
      "Chunk 65 processed in 409.81s\n",
      "Chunk 66 processed in 344.23s\n",
      "Chunk 67 processed in 340.77s\n",
      "Chunk 68 processed in 277.89s\n",
      "Chunk 69 processed in 270.89s\n",
      "Chunk 70 processed in 292.64s\n",
      "Chunk 71 processed in 262.58s\n",
      "Chunk 72 processed in 262.72s\n",
      "Chunk 73 processed in 291.64s\n",
      "Chunk 74 processed in 285.42s\n",
      "Chunk 75 processed in 382.39s\n",
      "Chunk 76 processed in 333.11s\n",
      "Chunk 77 processed in 380.81s\n",
      "Chunk 78 processed in 370.18s\n",
      "Chunk 79 processed in 357.01s\n",
      "Chunk 80 processed in 293.06s\n",
      "Chunk 81 processed in 274.66s\n",
      "Chunk 82 processed in 298.44s\n",
      "Chunk 83 processed in 353.68s\n",
      "Chunk 84 processed in 347.12s\n",
      "Chunk 85 processed in 447.51s\n",
      "Chunk 86 processed in 393.04s\n",
      "Chunk 87 processed in 377.64s\n",
      "Chunk 88 processed in 324.72s\n",
      "Chunk 89 processed in 279.75s\n",
      "Chunk 90 processed in 296.52s\n",
      "Chunk 91 processed in 246.97s\n",
      "Chunk 92 processed in 259.65s\n",
      "Chunk 93 processed in 284.74s\n",
      "Chunk 94 processed in 280.91s\n",
      "Chunk 95 processed in 380.49s\n",
      "Chunk 96 processed in 388.44s\n",
      "Chunk 97 processed in 417.97s\n",
      "Chunk 98 processed in 350.05s\n",
      "Chunk 99 processed in 311.90s\n",
      "Chunk 100 processed in 309.00s\n",
      "Chunk 101 processed in 300.09s\n",
      "Chunk 102 processed in 326.05s\n",
      "Chunk 103 processed in 325.17s\n",
      "Chunk 104 processed in 302.03s\n",
      "Chunk 105 processed in 418.24s\n",
      "Chunk 106 processed in 383.64s\n",
      "Chunk 107 processed in 370.52s\n",
      "Chunk 108 processed in 313.36s\n",
      "Chunk 109 processed in 294.31s\n",
      "Chunk 110 processed in 291.83s\n",
      "Chunk 111 processed in 275.23s\n",
      "Chunk 112 processed in 276.56s\n",
      "Chunk 113 processed in 261.45s\n",
      "Chunk 114 processed in 291.44s\n",
      "Chunk 115 processed in 390.06s\n",
      "Chunk 116 processed in 355.44s\n",
      "Chunk 117 processed in 577.70s\n",
      "Chunk 118 processed in 362.65s\n",
      "Chunk 119 processed in 347.13s\n",
      "Chunk 120 processed in 291.27s\n",
      "Chunk 121 processed in 187.64s\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/CLUSTER3_TRAIN_cleaned_cities.csv'\n",
    "\n",
    "distances_list = []\n",
    "chunk_size = 1_000_000\n",
    "print(\"Processing TRAIN data in chunks...\")\n",
    "chunk_idx = 0\n",
    "chunk_start_time = time.time()\n",
    "\n",
    "for chunk in pd.read_csv(train_file, usecols=columns_order + ['LC_CORINE'], chunksize=chunk_size):\n",
    "    chunk_idx += 1\n",
    "\n",
    "    # Scale numerical features\n",
    "    chunk_scaled = min_max_scale(chunk[columns_order].to_numpy(), min_vals, scale)\n",
    "    weighted_features = chunk_scaled * weights\n",
    "    lc_values = chunk['LC_CORINE'].values\n",
    "\n",
    "    # Initialize distances for this chunk\n",
    "    chunk_dists = np.full(len(chunk), np.inf)\n",
    "\n",
    "    # For each class in this chunk, query the corresponding KDTree\n",
    "    for lc_class in np.unique(lc_values):\n",
    "        if lc_class not in trees_by_class:\n",
    "            print('oops')\n",
    "            continue  # No matching test points\n",
    "\n",
    "        mask = lc_values == lc_class\n",
    "        query_points = weighted_features[mask]\n",
    "\n",
    "        # Query KDTree\n",
    "        dists, _ = trees_by_class[lc_class].query(query_points, k=1)\n",
    "        chunk_dists[mask] = dists.flatten()\n",
    "\n",
    "    # Save distances\n",
    "    distances_list.append(pd.DataFrame(chunk_dists, columns=['dist']))\n",
    "\n",
    "    duration = time.time() - chunk_start_time\n",
    "    print(f\"Chunk {chunk_idx} processed in {duration:.2f}s\")\n",
    "    chunk_start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc93bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All distances written to: results/CLUSTER3_TRAIN_min_dist.csv\n"
     ]
    }
   ],
   "source": [
    "# Write distances to CSV\n",
    "all_distances = pd.concat(distances_list, ignore_index=True)\n",
    "output_file = 'results/CLUSTER3_TRAIN_min_dist.csv'\n",
    "all_distances.to_csv(output_file, index=False)\n",
    "print(f\"\\nAll distances written to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
