{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8757d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sklearn\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a5c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['SP', 'RH', 'T_2M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "737a3df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting processing for TRAIN dataset...\n",
      "Processed TRAIN chunk 1 in 41.76 seconds.\n",
      "Processed TRAIN chunk 2 in 42.36 seconds.\n",
      "Processed TRAIN chunk 3 in 49.08 seconds.\n",
      "Processed TRAIN chunk 4 in 48.39 seconds.\n",
      "Processed TRAIN chunk 5 in 44.95 seconds.\n",
      "Processed TRAIN chunk 6 in 42.26 seconds.\n",
      "Processed TRAIN chunk 7 in 43.10 seconds.\n",
      "Processed TRAIN chunk 8 in 43.86 seconds.\n",
      "Processed TRAIN chunk 9 in 42.34 seconds.\n",
      "Processed TRAIN chunk 10 in 47.15 seconds.\n",
      "Processed TRAIN chunk 11 in 49.95 seconds.\n",
      "Processed TRAIN chunk 12 in 49.56 seconds.\n",
      "Processed TRAIN chunk 13 in 45.05 seconds.\n",
      "Processed TRAIN chunk 14 in 41.12 seconds.\n",
      "Processed TRAIN chunk 15 in 38.60 seconds.\n",
      "Processed TRAIN chunk 16 in 39.50 seconds.\n",
      "Processed TRAIN chunk 17 in 38.46 seconds.\n",
      "Processed TRAIN chunk 18 in 38.77 seconds.\n",
      "Processed TRAIN chunk 19 in 39.36 seconds.\n",
      "Processed TRAIN chunk 20 in 38.30 seconds.\n",
      "Processed TRAIN chunk 21 in 23.55 seconds.\n",
      "\n",
      "Starting processing for VALIDATION dataset...\n",
      "Processed VALIDATION chunk 1 in 22.54 seconds.\n",
      "\n",
      "Starting processing for TEST dataset...\n",
      "Processed TEST chunk 1 in 38.88 seconds.\n",
      "Processed TEST chunk 2 in 41.24 seconds.\n",
      "Processed TEST chunk 3 in 42.17 seconds.\n",
      "Processed TEST chunk 4 in 39.12 seconds.\n",
      "Processed TEST chunk 5 in 37.88 seconds.\n",
      "Processed TEST chunk 6 in 5.52 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "data_parts = ['TRAIN', 'VALIDATION', 'TEST']\n",
    "batch_size = 10_000_000\n",
    "\n",
    "\n",
    "# Process all datasets in chunks\n",
    "for part in data_parts:\n",
    "    cities_file = f'data/CLUSTER1_{part}_cleaned_cities.csv'\n",
    "    output_file = f'data/CLUSTER1_{part}_SH.csv'\n",
    "\n",
    "    print(f\"\\nStarting processing for {part} dataset...\")\n",
    "\n",
    "    cities_iter = pd.read_csv(cities_file, usecols=columns, chunksize=batch_size)\n",
    "\n",
    "    chunk_counter = 0\n",
    "    start_time = time.time() \n",
    "    for cities_chunk in cities_iter:\n",
    "        chunk_counter += 1\n",
    "        \n",
    "        e_s = 6.112 * np.exp((17.67*(cities_chunk['T_2M']-273.15))/(cities_chunk['T_2M'] -273.15 + 243.5))\n",
    "        e = cities_chunk['RH']/100 * e_s\n",
    "        cities_chunk['SH'] = 0.622 * e /(cities_chunk['SP']/100 - (1-0.622)*e)\n",
    "\n",
    "        # Append to output CSV\n",
    "        cities_chunk[['SH']].to_csv(output_file, mode='a', index=False, header=chunk_counter == 1)\n",
    "\n",
    "\n",
    "        # Calculate time taken for this chunk\n",
    "        chunk_time = time.time() - start_time\n",
    "        print(f\"Processed {part} chunk {chunk_counter} in {chunk_time:.2f} seconds.\")\n",
    "        start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609d487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007761358150904 0.0205094451356466\n"
     ]
    }
   ],
   "source": [
    "new = pd.read_csv('data/CLUSTER2_TEST_SH.csv')\n",
    "print(min(new['SH']), max(new['SH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f06f6418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007270362004324 0.0211888063360212\n"
     ]
    }
   ],
   "source": [
    "new = pd.read_csv('data/CLUSTER2_TRAIN_SH.csv')\n",
    "print(min(new['SH']), max(new['SH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b017ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011889979292697 0.0188543535228438\n"
     ]
    }
   ],
   "source": [
    "new = pd.read_csv('data/CLUSTER2_VALIDATION_SH.csv')\n",
    "print(min(new['SH']), max(new['SH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f522349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001609622531912 0.0178455094461258\n"
     ]
    }
   ],
   "source": [
    "new = pd.read_csv('data/CLUSTER3_TEST_SH.csv')\n",
    "print(min(new['SH']), max(new['SH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe64ff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001923270033781 0.0187589331509791\n"
     ]
    }
   ],
   "source": [
    "new = pd.read_csv('data/CLUSTER3_TRAIN_SH.csv')\n",
    "print(min(new['SH']), max(new['SH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74742536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000590144063 0.0161868354128747\n"
     ]
    }
   ],
   "source": [
    "new = pd.read_csv('data/CLUSTER3_VALIDATION_SH.csv')\n",
    "print(min(new['SH']), max(new['SH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5aadc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005002264751913 0.0198065411978871\n"
     ]
    }
   ],
   "source": [
    "new = pd.read_csv('data/CLUSTER1_TEST_SH.csv')\n",
    "print(min(new['SH']), max(new['SH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22ac6e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002805140023298 0.0232567166249862\n"
     ]
    }
   ],
   "source": [
    "new = pd.read_csv('data/CLUSTER1_TRAIN_SH.csv')\n",
    "print(min(new['SH']), max(new['SH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ccf568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011266362097941 0.0187219053889624\n"
     ]
    }
   ],
   "source": [
    "new = pd.read_csv('data/CLUSTER1_VALIDATION_SH.csv')\n",
    "print(min(new['SH']), max(new['SH']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
